# -*- coding: utf-8 -*-
"""Simulation Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DKMXlDks4zjeLj200GhjtN3qt8fyRrAH

## Imports
"""

# Commented out IPython magic to ensure Python compatibility.
#@title Mounting Google Drive
from google.colab import files
from google.colab import drive

from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My\ Drive/Y4/Research \Project

#@title Importing libraries
import numpy as np
from numpy.random import RandomState
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
!pip install -U seaborn

from openpyxl import load_workbook
import datetime
import statistics

!pip install simpy
import simpy 

import scipy
import warnings
import scipy.stats as st
import statsmodels as sm

from statsmodels.distributions.empirical_distribution import ECDF
import random
from bisect import bisect_left

#@title Auxiliary functions

# common
def plot_two_lists(nameX, x, nameY, y, kind_type, height=7):
  # plotting the graph
  df = pd.DataFrame()
  df[nameX] = x
  df[nameY] = y
  
  sns.set_style("white")  
  sns.relplot(x=nameX, y=nameY, data=df, kind=kind_type, 
              height=height, palette="b", color="#ff4f4f")
#   plt.boxplot(y)
#   plt.savefig('SAVED/alert_count_2.png', dpi=100)
  
def print_stats(local_list):
  try:
    print("Max:", max(local_list))
    print("Min:", min(local_list))
    print("\nMean: %.2f" % statistics.mean(local_list))
    print("Median:", statistics.median(local_list))
  except Exception: # in case there is only 1 value
    print("Max: N/A")
    print("Min: N/A")
    print("Mean: N/A")
    print("Median: N/A")
  
  

#   print("Median High:", statistics.median_high(local_list))
#   print("Median Low:", statistics.median_low(local_list))
  try:
    print("Mode:", statistics.mode(local_list))
  except Exception: # in case all values are equally chanced
    print("Mode: no mode.")
  
# for analysing bug creation
def count_vs_days(days_between_bugs):
  # getting the count for distibution "Count vs Days"
  new_days_between_bugs = []
  count_repeated_days = []
  # making the first append
  new_days_between_bugs.append(days_between_bugs[0])
  temp_count = 1
  count_repeated_days.append(temp_count)
  # continue
  for i in range(1, len(days_between_bugs), 1): # starting with 1 not 0, as the first value is already appended
    if days_between_bugs[i] == new_days_between_bugs[-1]:
      temp_count += 1
      count_repeated_days[-1] = temp_count
    else:
      new_days_between_bugs.append(days_between_bugs[i])
      temp_count = 1
      count_repeated_days.append(temp_count)
  
  return count_repeated_days, new_days_between_bugs

"""# Playing with the data

## Analysing Alert Count

### Basic analysis of alert count
"""

#@title Analysing alert counts
def show_alert_counts():
  workbook_git = load_workbook('data/git-commits.xlsx')
  sheet_git = workbook_git['Records'] # print(workbook_git.sheetnames)

  list_of_count_alerts = []
  non_zero_values = 0
  for count_alerts in sheet_git['G']: # column G is for Alert Count
    if count_alerts != sheet_git['G1']:
      if count_alerts.value != -1: # ignoring -1, which is irrelevant
        list_of_count_alerts.append(count_alerts.value)
      
      # getting the non-zero values
      if count_alerts.value > 0:
        non_zero_values += 1
  
  print_stats(list_of_count_alerts)
  
  plot_two_lists("Commit #", range(len(list_of_count_alerts)), "Alert Count", list_of_count_alerts, "line")
  
  percentage_non_zero_values = (non_zero_values / len(list_of_count_alerts)) * 100
  print("Percentage of non-zero values: %.2f" % percentage_non_zero_values, end="%")
  
show_alert_counts()

#@title Analysing alert counts, kludges and 0 contribution commits
# pd.DataFrame(list_of_count_alerts).quantile([0.25,0.5,0.75])

# set the threshold to be (# of lines changed / # of alerts) > eg 100
# meaning every 100 lines were changed without an error
# avoid division by 0 lol

# kludge = 0
# for count_alerts in list_of_count_alerts:
#   if count_alerts:
#     pass

# make a distribution PER developer that he will commit a kludge or a fix






alert_count_threshold = 0 # lines before an error
# major_commit_threshold = 100

def show_alert_counts_():
  workbook = load_workbook('data/git-commits.xlsx')
  sheet = workbook['Records']

  list_of_count_alerts = []
  
  kludges = 0
#   significant_commits = 0

  # starting with index 2 as 0 represents overview of all values and 1 stands for the heading
  for i in range(2, len(sheet['G'])+1, 1):
    # G for Alert Count, I for Lines Added
    # ignore 0 contribution commits
    if sheet['G'+str(i)].value != -1 and sheet['I'+str(i)].value > 0:
      list_of_count_alerts.append(sheet['G'+str(i)].value)
      
      if sheet['G'+str(i)].value != 0:
        quality_control = sheet['I'+str(i)].value / sheet['G'+str(i)].value # lines per alert
        
        if quality_control > alert_count_threshold:
          kludges += 1
          
#       if sheet['I'+str(i)].value > major_commit_threshold:
#         major_commits += 1
  
  
#   print("Major commits:", major_commits, "where kludges", (kludges / major_commits * 100), "%")
  print("Kludges:", kludges, "which is", (kludges / len(list_of_count_alerts) * 100), "%\n")
  
  print(len(list_of_count_alerts))
  print_stats(list_of_count_alerts)
  plot_two_lists("Commit #", range(len(list_of_count_alerts)), "Alert Count", list_of_count_alerts, "line")
  
show_alert_counts_()

"""### Advanced developer analysis"""

# #@markdown ### Enter the minimal number of commits a developer must make
# minimal_number_of_commits = 0 #@param {type:"integer"}

#@markdown ### Enter the acceptable proportion of alerts per line of code
#@markdown The bigger the number, the easier it is to avoid a kludge. 0 means all alerts are kludges. 1 means there were as many alerts as there were added new lines.
quality_of_code = 0.03 #@param {type:"slider", min:0, max:1, step:0.01}

#@markdown ### At least how many lines a commit must be (in terms of lines added)
#@markdown Commits smaller than this number of lines will be ignored.
minimal_number_of_lines = 10 #@param {type:"integer"}
maximal_number_of_lines = 900 #@param {type:"integer"}

# 0.03 . 10 . 900

#@title Get developers' fix / kludge distribution
# checking the developers involved:
# check the developers in overview who made commits to github
# filter devs who had less than median commits
# get distribution for each dev

class Developers():
  # get unique emails for each developer
  def get_emails(self):
    list_of_devs = []
    
    # starting with index 2 as 0 represents overview of all values and 1 stands for the heading
    for i in range(2, len(self.sheet['B'])+1, 1):
      value = self.sheet['B'+str(i)].value
      value = value.split("<")
      value = value[1].split(">")
      list_of_devs.append(value[0])

    # getting unique values
    list_of_devs = list(set(list_of_devs))
#     for i in list_of_devs:
#       print(i)  
#     print(len(list_of_devs))

    return list_of_devs

  def get_developers(self):
    self.kludges = []
    self.fixes = []
    
    # tuple of: email address, number of total commits, number of kludges
    dev_tuple = []
    for email in self.emails:
      dev_tuple.append([email, 0, 0]) # initialise for each unique email
      
      for i in range(2, len(self.sheet['B'])+1, 1):
        if email in self.sheet['B'+str(i)].value and self.sheet['I'+str(i)].value > minimal_number_of_lines and self.sheet['I'+str(i)].value < maximal_number_of_lines:
          dev_tuple[-1][1] += 1 # increase the count of total commits
          
          alert_per_line = 0
          if self.sheet['I'+str(i)].value > 0: # alert count divided by lines added
            alert_per_line = self.sheet['G'+str(i)].value / self.sheet['I'+str(i)].value
          
          bug_ids = self.sheet['H'+str(i)].value.split("\n")
          if alert_per_line > quality_of_code:
            dev_tuple[-1][2] += 1 # increase the count of total kludges
            
            if bug_ids is not None:
              for bug_id in bug_ids: 
                if bug_id != "":
                  self.kludges.append([bug_id, self.sheet['I'+str(i)].value])
          else:
            if bug_ids is not None:
              for bug_id in bug_ids: 
                if bug_id != "":
                  self.fixes.append([bug_id, self.sheet['I'+str(i)].value])
    
    return dev_tuple
  
  def filter_out_low_contribution(self, dev_stats):
    commits_by_devs = [x[1] for x in dev_stats]
    print_stats(commits_by_devs)
    median = statistics.median(commits_by_devs)
    
    new_dev_stats = []
    for x in dev_stats:
      if x[1] > median:
         new_dev_stats.append(x)
    
    print("\nTotal number of developers:", len(new_dev_stats), "\n")
#     for i in new_dev_stats:
#       print(i)
      
    return new_dev_stats
      
  def get_distribution(self, devs):
    global_devs = []
    new_devs = []
    for dev in devs:
      distr = dev[2] / dev[1]
      new_devs.append([dev[0], distr])
      global_devs.append([dev, new_devs[-1]])
      
    for i in global_devs:
      print(i)
      
    return new_devs
    
  
  def __init__(self):
    workbook = load_workbook('data/git-commits.xlsx')
    self.sheet = workbook['Records']
    self.lines_added = []
    
    self.emails = self.get_emails()
    dev_stats_temp = self.get_developers()
    self.dev_stats_full = dev_stats_temp # saving unfiltered developers just in case
    dev_stats_temp = self.filter_out_low_contribution(dev_stats_temp)
    self.dev_stats = self.get_distribution(dev_stats_temp)
    
  def return_devs(self):
    return self.dev_stats
  
  def return_kludges(self):
    return [x[0] for x in self.kludges]
  def return_fixes(self):
    return [x[0] for x in self.fixes]
  
  def return_kludges_size(self):
    return [x[1] for x in self.kludges]
  def return_fixes_size(self):
    return [x[1] for x in self.fixes]
  
  def return_full_dev_stats(self):
    return self.dev_stats_full
  
#   def return_lines_added(self):
#     return self.lines_added
    
developers = Developers()

# print(len(developers.return_kludges()),len(developers.return_fixes()))

"""## Modelling Bug Creation"""

#@title Do you want to use float days for better precision?
use_float_days = True #@param {type:"boolean"}

#@title Analysing bug creation: consecutive "creation_date"
def show_bug_creation():
  workbook = load_workbook('data/bugzilla-overview.xlsx')
  sheet = workbook['Bugs Overview'] # print(workbook.sheetnames)

  # getting the bug creation dates
  bug_creation_dates = []
  for bug_creation_date in sheet['I']: # column I is for Creation Time
    if bug_creation_date != sheet['I1']:
      date_formatted = datetime.datetime.strptime(bug_creation_date.value,"%Y-%m-%dT%H:%M:%SZ")
      bug_creation_dates.append(date_formatted)
  bug_creation_dates.sort() # needs sorting
  
  # getting the days in between consecutive bugs
  days_between_bug_creation = []
  for i in range(len(bug_creation_dates)-1):
    time_delta = bug_creation_dates[i+1] - bug_creation_dates[i]
    
    if use_float_days == True:
      days_as_a_float = time_delta.days + (time_delta.seconds / (24*60*60) )
    else:
      days_as_a_float = time_delta.days
    
    days_between_bug_creation.append(days_as_a_float)
#   print(days_between_bug_creation)
  return_value = days_between_bug_creation.copy()
  days_between_bug_creation.sort() # needed to model the distribution
  
  print("(Number of days to create a new bug)")
  print_stats(days_between_bug_creation)
  
  count_repeated_days, new_days_between_bug_creation = count_vs_days(days_between_bug_creation)
  plot_two_lists('Days', new_days_between_bug_creation, 'Count', count_repeated_days, 'line')
  
#   print("\n", len(return_value), return_value)
  return return_value
  
  
global_created_bugs = show_bug_creation()

#@markdown Playing with the code
# workbook_bug_overview = load_workbook('data/bugzilla-overview.xlsx')
# sheet_bug_overview = workbook_bug_overview['Bugs Overview']
# # print(workbook_bug_overview.sheetnames)

# # getting the bug creation dates
# bug_creation_dates = []
# for bug_creation_date in sheet_bug_overview['I']: # column I is for Creation Time
#   if bug_creation_date != sheet_bug_overview['I1']:
#     bug_creation_dates.append(bug_creation_date.value)
# bug_creation_dates.sort()
# bug_creation_dates

#-------------------------------------------------------------------------------

# test_two = list_of_count_alerts
# test_three = []
# for date in test_two:
#   date_formatted = datetime.datetime.strptime(date,"%Y-%m-%dT%H:%M:%SZ")
#   test_three.append(date_formatted)
  
# test_three

#-------------------------------------------------------------------------------

# av_days = []
# for i in range(len(test_three)-1):
#   time_delta = test_three[i+1]-test_three[i]
#   av_days.append(time_delta.days)
# av_days

#-------------------------------------------------------------------------------

# print("%.2f" % (sum(av_days) / len(av_days)))

#-------------------------------------------------------------------------------

# av_days_fake = []
# days_temp = []

# av_days_fake = av_days

# av_days_fake.append(10)
# av_days_fake.append(43)
# av_days_fake.append(27)
# av_days_fake.append(27)

# days_temp = av_days_fake
# days_temp.sort()
# days_temp

#-------------------------------------------------------------------------------

# test_four = []
# count = []

# # making the first append
# test_four.append(days_temp[0])
# temp_count = 1
# count.append(temp_count)
# # continue
# for i in range(1, len(days_temp), 1): # starting with 1 not 0, as the first value is already appended
#   if days_temp[i] == test_four[-1]:
#     temp_count += 1
#     count[-1] = temp_count
#   else:
#     test_four.append(days_temp[i])
#     temp_count = 1
#     count.append(temp_count)
    
# # checking the results
# print("C:", len(count), "T:", len(test_four))
# # for i in range(len(test_four)):
# #   print(count[i], test_four[i])

#-------------------------------------------------------------------------------

# df = pd.DataFrame()
# df['Count'] = count
# df['Days'] = test_four
# sns.relplot(x='Days', y='Count', data=df, kind="line")
# plt.show()

#-------------------------------------------------------------------------------

# test = list_of_count_alerts

# # indices = sorted(range(len(test)), key=lambda k: test[k], reverse = True)
# # for i in test:
# #   print(i[0:4])
# # test.sort(key=lambda k: indices[k])
# test.sort()
# test

"""## Modelling Bug Termination

### Bug Fixation
"""

#@title Analysing bug fixation: "creation_date - last_change_time"
def show_bug_fixation_():
  workbook = load_workbook('data/bugzilla-overview.xlsx')
  sheet = workbook['Bugs Overview'] # print(workbook.sheetnames)

  # getting the bug termination dates: depending on Status and Is Open
  bug_fixation_dates = []
  for i in range(2, len(sheet['K'])+1, 1): # starting with index 2 as 0 represents overview of all values and 1 stands for the heading
#     print(sheet['O'+str(i)].value, sheet['J'+str(i)].value)
    if sheet['O'+str(i)].value == "RESOLVED" and sheet['J'+str(i)].value == "false": # K for Last Changed Time, O for Status, J for Is Open
      date_formatted_lct = datetime.datetime.strptime(sheet['K'+str(i)].value,"%Y-%m-%dT%H:%M:%SZ") # add the corresponding date
      date_formatted_ct = datetime.datetime.strptime(sheet['I'+str(i)].value,"%Y-%m-%dT%H:%M:%SZ") 
      time_delta = date_formatted_lct - date_formatted_ct
      if time_delta.days != -1:
        bug_fixation_dates.append(time_delta.days)
      else:
        print("-1:",i, date_formatted_lct, date_formatted_ct)
  return_value = bug_fixation_dates.copy()
  bug_fixation_dates.sort() # needed to model the distribution
    
  print("\n(Number of days to fix a bug)")
  print_stats(bug_fixation_dates)
        
  count_repeated_days, new_days_between_bugs = count_vs_days(bug_fixation_dates)
  plot_two_lists('Days', new_days_between_bugs, 'Count', count_repeated_days, 'line')
  
  return return_value
  
#   df = pd.DataFrame()
#   df["Days"] = [str(x) for x in new_days_between_bugs]
#   df["Count"] = count_repeated_days
#   sns.set(rc={'figure.figsize':(15, 100)})
#   sns.set_style("white")
#   sns.relplot(x="Days", y="Count", data=df, kind="scatter")
#   plt.show()
  
global_fixed_bugs_ = show_bug_fixation_() 
# print(global_fixed_bugs_)

#@title Analysing bug fixation: consecutive "last_change_time"
def show_bug_fixation():
  workbook = load_workbook('data/bugzilla-overview.xlsx')
  sheet = workbook['Bugs Overview'] # print(workbook.sheetnames)
  
  # getting the bug termination dates: depending on Status and Is Open
  bug_fixation_dates = []
  for i in range(2, len(sheet['K'])+1, 1): # starting with index 2 as 0 represents overview of all values and 1 stands for the heading
    if sheet['O'+str(i)].value == "RESOLVED" and sheet['J'+str(i)].value == "false": # K for Last Changed Time, O for Status, J for Is Open
      date_formatted = datetime.datetime.strptime(sheet['K'+str(i)].value,"%Y-%m-%dT%H:%M:%SZ")
      bug_fixation_dates.append(date_formatted)
  bug_fixation_dates.sort() # needs sorting
#   print(bug_fixation_dates)
  
  # getting the days in between consecutive bugs
  days_between_bug_fixation = []
  for i in range(len(bug_fixation_dates)-1):
    time_delta = bug_fixation_dates[i+1] - bug_fixation_dates[i]
    if use_float_days == True:
      days_as_a_float = time_delta.days + (time_delta.seconds / (24*60*60) )
    else:
      days_as_a_float = time_delta.days
    
    days_between_bug_fixation.append(days_as_a_float)
  return_value = days_between_bug_fixation.copy()
  days_between_bug_fixation.sort() # needed to model the distribution
    
  print("(Number of days to fix a bug)")
  print_stats(days_between_bug_fixation)
        
  count_repeated_days, new_days_between_bugs = count_vs_days(days_between_bug_fixation)
  plot_two_lists('Days', new_days_between_bugs, 'Count', count_repeated_days, 'line')
  
  return return_value
    
global_fixed_bugs = show_bug_fixation() 
# print(len(global_fixed_bugs), global_fixed_bugs)

"""### Bug Resolution"""

# third one: use history to track "OPEN" - "CLOSED"
#@title Analysing bug resolution: "OPENED" - "RESOLVED"

class OverallBugResolutionTime():
  def get_resolved_reopened_status(self):
    workbook = load_workbook('data/bugzilla-history.xlsx')
    sheet = workbook['Bugs History']

  #   # getting unique bugs
  #   all_bugs = []
  #   for i in range(2, len(sheet['B'])+1, 1): all_bugs.append(sheet['B'+str(i)].value)
  #   all_bugs = list(set(all_bugs))

  #   new_all_bugs = []
  #   for i in all_bugs:
  #     new_all_bugs.append([i, 0, 0])

    # getting unique tuples: bug OPEN and CLOSED
    all_bugs_tuples = [] # B for bugID, E for date, F for added
    for i in range(2, len(sheet['B'])+1, 1):
      # getting ESOLVED / REOPEN status
      if sheet['F'+str(i)].value == "RESOLVED" or sheet['F'+str(i)].value == "REOPENED":
  #       all_bugs.remove(sheet['B'+str(i)].value)
        all_bugs_tuples.append([int(sheet['B'+str(i)].value), 
                      sheet['E'+str(i)].value,
                      sheet['F'+str(i)].value])

  #     if sheet['F'+str(i)].value == "RESOLVED":
  #       index = all_bugs.index(sheet['B'+str(i)].value)
  #       entry = new_all_bugs[index]
  #       entry[1] = 1

  #     if sheet['G'+str(i)].value == "cc":
  #       index = all_bugs.index(sheet['B'+str(i)].value)
  #       entry = new_all_bugs[index]
  #       entry[2] = 1

  #   for i in all_bugs_tuples:
  #     print(i)

  #   all_bugs_temp = [x[0] for x in new_all_bugs]
  #   for i in range(len(new_all_bugs)):
  #     print(new_all_bugs[i])
  #     print(all_bugs_temp[i])

    return all_bugs_tuples


  def get_dif_start_end(self, start, end):
    df_start = datetime.datetime.strptime(start[1],"%Y-%m-%dT%H:%M:%SZ")
    df_end = datetime.datetime.strptime(end[1],"%Y-%m-%dT%H:%M:%SZ")
    time_delta = df_end - df_start

    days_as_a_float = 0
    if use_float_days == True:
      days_as_a_float = time_delta.days + (time_delta.seconds / (24*60*60) )
    else:
      days_as_a_float = time_delta.days

    return days_as_a_float


  def get_resolution_periods(self, history):
    workbook = load_workbook('data/bugzilla-overview.xlsx')
    sheet = workbook['Bugs Overview']

  #   dif = 2
  #   for i in range(2, len(sheet['B'])+1, 1):
  #     if sheet['O'+str(i)].value != "RESOLVED":
  #       dif += 1
  #     elif sheet['B'+str(i)].value == history[i-dif][0]:
  #       print("Good")
  #     else:
  #       print("BAD", i, sheet['B'+str(i)].value, history[i-dif][0])

    all_bugs = [] # B for bugID, I for creation time
    for i in range(2, len(sheet['B'])+1, 1):
      # checking for duplicate data, which shouldn't be the case but appears for some reason!!
      can_add = True
      if all_bugs: # if list is non-empty 
        if int(sheet['B'+str(i)].value) == all_bugs[-1][0]:
          can_add = False

      if sheet['O'+str(i)].value in ["NEW", "ASSIGNED", ''] or sheet['O'+str(i)].value is None: 
        can_add = False

      if can_add == True:
        all_bugs.append([int(sheet['B'+str(i)].value), 
                        sheet['I'+str(i)].value,
                        sheet['M'+str(i)].value])

    all_bugs.sort(key=lambda tup: tup[0])

    history.sort(key=lambda tup: tup[1]) # sort by time
    history.sort(key=lambda tup: tup[0]) # sort by bugID

    # check all tuples are "RESOLVED -> REOPENED -> RESOLVED ..."
    indeces_to_delete = []
    for i in range(1, len(history), 1):
      if history[i][0] == history[i-1][0] and history[i][2] == "RESOLVED" and history[i-1][2] == "RESOLVED":
        print("*** ERROR, two resolved:", history[i], history[i-1])      
      if not history[i][0] == history[i-1][0] and history[i-1][2] == "REOPENED":
  #       print("REOPENED but never RESOLVED", history[i-1])
        indeces_to_delete.append(i)
    # delete REOPENED but not RESOLVED afterwards
    indeces_to_delete.sort(reverse = True)
    for i in indeces_to_delete:
      del history[i-1]

    for i in range(1, len(all_bugs), 1):
      if all_bugs[i][0] == all_bugs[i-1][0]:
        print(all_bugs[i], all_bugs[i-1])
  #         del all_bugs[i]

  #         can_break = False
  #         break

  #       if i == len(all_bugs):
  #         can_break = True

  #     if can_break == True:
  #       break

    all_periods = []
    for i in range(len(all_bugs)):
      start = all_bugs[0]
      end = history[0]
  #     print(start, end, len(all_bugs), len(history))

      if end[2] == "RESOLVED" and start[0] == end[0]:
        days_as_a_float = self.get_dif_start_end(start, end)
        all_periods.append(days_as_a_float)
  #       print(days_as_a_float)

        del history[0]
        if len(history) > 1:
          while history[0][0] == all_bugs[0][0] and history[0][2] == "REOPENED":
            start = history[0] # reopened
            end = history[1] # resolved

            if end[0] == start[0] and end[2] == "RESOLVED":
              days_as_a_float = self.get_dif_start_end(start, end)
              all_periods.append(days_as_a_float)
  #             print(days_as_a_float)

              del history[0]
              del history[0]

            else:
              print("*1* ERROR:", start, end)

              del history[0]

            if len(history) > 1:
              continue
            else:
              break

        del all_bugs[0]  


      else:
        print("*2* ERROR:", start, end)
  #       break
        del all_bugs[0]

    print("Remained:", len(all_bugs), len(history), "\n")

    return all_periods



  def show_res_time(self, list_of_res):
    print("(Number of days to resolve a bug)")
    print_stats(list_of_res)

  #   count_repeated_days, new_days_between_bugs = count_vs_days(list_of_res)
  #   plot_two_lists('Days', new_days_between_bugs, 'Count', count_repeated_days, 'line')
  
  def __init__(self):
    get_resolved_reopened_status = self.get_resolved_reopened_status()
    global_resolution_time = self.get_resolution_periods(get_resolved_reopened_status)

    # removing -ve numbers, which is a weird behaviour by Bugzilla
    for i in global_resolution_time:
      if i < 0:
        global_resolution_time.remove(i)

    self.show_res_time(global_resolution_time)
    self.global_resolution_time = global_resolution_time
    
  def return_res(self):
    return self.global_resolution_time
    
global_resolution_time = OverallBugResolutionTime().return_res()

"""### Bug Resolution 2: Added Fix and Kludge, and Reopen Chance"""

#@markdown ### Do you wish to remove duplicates?

yes_or_no = True #@param {type:"boolean"}

global_devs_fix = developers.return_fixes().copy()
global_devs_kludge = developers.return_kludges().copy()

print("Previous sizes",len(global_devs_fix), len(global_devs_kludge))
if yes_or_no:
  for i in global_devs_kludge:
    if i in global_devs_fix:
      global_devs_fix.remove(i)
      global_devs_kludge.remove(i)
print("Current sizes",len(global_devs_fix), len(global_devs_kludge))

#@title Analysing bug resolution: "OPENED" - "RESOLVED" per FIX or per KLUDGE
# Added knowledge: needs to know which bug is a Fix, and which is a Kludge

class ResolutionTime():
  def reopen_chance(self):
    self.total_resolved = 0
    self.total_reopened = 0
    
#   def reopen_chance_fk(self):
#     self.fix_reopen = 0
#     self.fix_resolved = 0
    
#     self.kludge_reopen = 0
#     self.kludge_resolved = 0
  
  def get_resolved_reopened_status(self, bug_type):
    workbook = load_workbook('data/bugzilla-history.xlsx')
    sheet = workbook['Bugs History']

    # getting tuples: bugID, date and added
    all_bugs_tuples = [] # B for bugID, E for date, F for added

    for i in range(2, len(sheet['B'])+1, 1):
#       print(str(sheet['B'+str(i)].value))
      if str(int(sheet['B'+str(i)].value)) in bug_type:
        # getting RESOLVED / REOPEN status
        if sheet['F'+str(i)].value == "RESOLVED" or sheet['F'+str(i)].value == "REOPENED":
          all_bugs_tuples.append([int(sheet['B'+str(i)].value), 
                        sheet['E'+str(i)].value,
                        sheet['F'+str(i)].value])
          
          if sheet['F'+str(i)].value == "RESOLVED": self.total_resolved += 1
          if sheet['F'+str(i)].value == "REOPENED": self.total_reopened += 1
    
    return all_bugs_tuples


  def get_diff_start_end(self, start, end):
    df_start = datetime.datetime.strptime(start[1],"%Y-%m-%dT%H:%M:%SZ")
    df_end = datetime.datetime.strptime(end[1],"%Y-%m-%dT%H:%M:%SZ")
    time_delta = df_end - df_start

    days_as_a_float = 0
    if use_float_days == True:
      days_as_a_float = time_delta.days + (time_delta.seconds / (24*60*60) )
    else:
      days_as_a_float = time_delta.days

    return days_as_a_float


  def get_resolution_periods(self, history, bug_type):
#     self.reopen_chance() # initiating the distribution
    
    workbook = load_workbook('data/bugzilla-overview.xlsx')
    sheet = workbook['Bugs Overview']

    all_bugs = [] # B for bugID, I for creation time
    for i in range(2, len(sheet['B'])+1, 1):
      if str(int(sheet['B'+str(i)].value)) in bug_type:
        # checking for duplicate data, which shouldn't be the case but appears for some reason!!
        can_add = True
        if all_bugs: # if list is non-empty 
          if int(sheet['B'+str(i)].value) == all_bugs[-1][0]:
            can_add = False

        # excluding unfinished statuses
        if sheet['O'+str(i)].value in ["NEW", "ASSIGNED", ''] or sheet['O'+str(i)].value is None: 
          can_add = False

        # adding only what's needed
        if can_add == True:
          all_bugs.append([int(sheet['B'+str(i)].value), 
                          sheet['I'+str(i)].value,
                          sheet['M'+str(i)].value])

    all_bugs.sort(key=lambda tup: tup[0])

    history.sort(key=lambda tup: tup[1]) # sort by time
    history.sort(key=lambda tup: tup[0]) # sort by bugID

    # check all tuples are "RESOLVED -> REOPENED -> RESOLVED ..." i.e. finishing with "RESOLVED"
    indeces_to_delete = []
    for i in range(1, len(history), 1):
      if history[i][0] == history[i-1][0] and history[i][2] == "RESOLVED" and history[i-1][2] == "RESOLVED":
        print("*** ERROR, two resolved:", history[i], history[i-1])      
      if not history[i][0] == history[i-1][0] and history[i-1][2] == "REOPENED":
        indeces_to_delete.append(i)
    
    print("Got:", len(all_bugs), len(history))
    # delete REOPENED but not RESOLVED afterwards
    indeces_to_delete.sort(reverse = True)
    for i in indeces_to_delete:
      del history[i-1]

    for i in range(1, len(all_bugs), 1):
      if all_bugs[i][0] == all_bugs[i-1][0]: print(all_bugs[i], all_bugs[i-1])

    all_periods = []
    for i in range(len(all_bugs)):
      start = all_bugs[0]
      end = history[0]

      if end[2] == "RESOLVED" and start[0] == end[0]:
        days_as_a_float = self.get_diff_start_end(start, end)
        all_periods.append(days_as_a_float)
        del history[0]
        
        if len(history) > 1:
          while history[0][0] == all_bugs[0][0] and history[0][2] == "REOPENED":
#             self.total_reopened += 1
            start = history[0] # reopened
            end = history[1] # resolved

            if end[0] == start[0] and end[2] == "RESOLVED":
              days_as_a_float = self.get_diff_start_end(start, end)
              all_periods.append(days_as_a_float)
              del history[0]
              del history[0]

            else:
              print("*1* ERROR:", start, end)
              del history[0]

            if len(history) > 1: continue
            else: break

#         self.total_resolved += 1
        del all_bugs[0]  

      else:
        print("*2* ERROR:", start, end)
        del all_bugs[0]

    print("Remained:", len(all_bugs), len(history), "\n")
    return all_periods

  def show_res_time(self, list_of_res):
    print("(Number of days to resolve a bug)")
    print_stats(list_of_res)
    
  def remove_negative_values(self, data):
    for i in data:
      if i < 0:
        data.remove(i)
    return data
  
  def __init__(self):  
    self.reopen_chance() # initiating the distribution
    statuses_fixes = self.get_resolved_reopened_status(global_devs_fix)
    statuses_kludges = self.get_resolved_reopened_status(global_devs_kludge)

        
    res_time_fixes = self.get_resolution_periods(statuses_fixes, global_devs_fix)
#     self.chance_of_reopen_f = self.total_reopened / self.total_resolved # calculating the results    
    res_time_kludges = self.get_resolution_periods(statuses_kludges, global_devs_kludge)
#     self.chance_of_reopen_k = self.total_reopened / self.total_resolved # calculating the results
    
    self.chance_of_reopen = self.total_reopened / self.total_resolved # calculating the results
    
    # removing -ve numbers, which is a weird behaviour by Bugzilla
    self.new_res_time_fixes = self.remove_negative_values(res_time_fixes)
    self.new_res_time_kludges = self.remove_negative_values(res_time_kludges)

#     print("\n----------------------------------")
#     print("FIXES:")
#     print("----------------------------------")
#     self.show_res_time(self.new_res_time_fixes)
    
#     print("\n----------------------------------")
#     print("KLUDGES:")
#     print("----------------------------------")
#     self.show_res_time(self.new_res_time_kludges)
    self.global_resolution_time = global_resolution_time
    
    print("Chance of REOPEN:", self.chance_of_reopen)
#     print("\nChance of REOPEN FIX:", self.chance_of_reopen_f)
#     print("Chance of REOPEN KLUDGE:", self.chance_of_reopen_k)
    
  def return_res_fixes(self): 
    return self.new_res_time_fixes
  
  def return_res_kludges(self): 
    return self.new_res_time_kludges
  
  def return_reopen_chance(self): 
    return self.chance_of_reopen
  
#   def return_reopen_chance_f(self): 
#     return self.chance_of_reopen_f
  
#   def return_reopen_chance_k(self): 
#     return self.chance_of_reopen_k
    
res_time_per_fix_kludge = ResolutionTime()

global_fixes_periods = res_time_per_fix_kludge.return_res_fixes()
global_kludges_periods = res_time_per_fix_kludge.return_res_kludges()
global_chance_of_reopen = res_time_per_fix_kludge.return_reopen_chance()
# global_chance_of_reopen_f = res_time_per_fix_kludge.return_reopen_chance_f()
# global_chance_of_reopen_k = res_time_per_fix_kludge.return_reopen_chance_k()

#@title Plotting "Fix vs Fludge" resolution period

print("Mean difference:", (statistics.mean(global_fixes_periods) / statistics.mean(global_kludges_periods)))
print("Median difference:", (statistics.median(global_fixes_periods) / statistics.median(global_kludges_periods)))

print("\n----------------------------------")
print("FIXES:")
print("----------------------------------")
print_stats(global_fixes_periods)

print("\n----------------------------------")
print("KLUDGES:")
print("----------------------------------")
print_stats(global_kludges_periods)

plt.figure(figsize=(12,10))
data_to_plot = [global_fixes_periods, global_kludges_periods]
plt.boxplot(data_to_plot)
plt.savefig('SAVED/resolution_boxplot.png', dpi=250)
plt.show()

#@title Plotting "Fix vs Fludge" size (lines added)
plt.figure(figsize=(12,10))

global_kludge_sizes = developers.return_kludges_size().copy()
global_fix_sizes = developers.return_fixes_size().copy()


print("Mean difference:", (statistics.mean(global_fix_sizes) / statistics.mean(global_kludge_sizes)))
print("Median difference:", (statistics.median(global_fix_sizes) / statistics.median(global_kludge_sizes)))

print("\n----------------------------------")
print("FIXES:")
print("----------------------------------")
print_stats(global_fix_sizes)

print("\n----------------------------------")
print("KLUDGES:")
print("----------------------------------")
print_stats(global_kludge_sizes)
data_to_plot = [global_fix_sizes, global_kludge_sizes]
plt.boxplot(data_to_plot)
plt.savefig('SAVED/size_boxplot.png', dpi=100)
plt.show()

#@title Plotting "Fix vs Fludge" time per line of code
plt.figure(figsize=(12,10))

global_kludge_tpl = []
global_fix_tpl = []

for i in range(len(global_fixes_periods)):
  global_fix_tpl.append(global_fixes_periods[i] / global_fix_sizes[i])

for i in range(len(global_kludges_periods)):
  global_kludge_tpl.append(global_kludges_periods[i] / global_kludge_sizes[i])

print("Mean difference:", (statistics.mean(global_fix_tpl) / statistics.mean(global_kludge_tpl)))
print("Median difference:", (statistics.median(global_fix_tpl) / statistics.median(global_kludge_tpl)))


print("\n----------------------------------")
print("FIXES:")
print("----------------------------------")
print_stats(global_fix_tpl)

print("\n----------------------------------")
print("KLUDGES:")
print("----------------------------------")
print_stats(global_kludge_tpl)
data_to_plot = [global_fix_tpl, global_kludge_tpl]
plt.boxplot(data_to_plot)
plt.show()

"""# Bug Simulation

## Fitting the distributions
"""

#@title Using empirical model by default (tick for theoretical)
use_theoretical_models = True #@param {type:"boolean"}

# initialise values
bug_creation_best_fit, bug_creation_bf_params = st.norm, st.norm.fit(global_created_bugs)
bug_fixation_best_fit, bug_fixation_bf_params = st.norm, st.norm.fit(global_fixed_bugs)

#@title Checking whether theoretical distributions can fit
# Distributions to check
DISTRIBUTIONS = [        
    st.alpha,st.anglit,st.arcsine,st.beta,st.betaprime,st.bradford,st.burr,st.cauchy,st.chi,st.chi2,st.cosine,
    st.dgamma,st.dweibull,st.erlang,st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.fatiguelife,st.fisk,
    st.foldcauchy,st.foldnorm,st.frechet_r,st.frechet_l,st.genlogistic,st.genpareto,st.gennorm,st.genexpon,
    st.genextreme,st.gausshyper,st.gamma,st.gengamma,st.genhalflogistic,st.gilbrat,st.gompertz,st.gumbel_r,
    st.gumbel_l,st.halfcauchy,st.halflogistic,st.halfnorm,st.halfgennorm,st.hypsecant,st.invgamma,st.invgauss,
    st.invweibull,st.johnsonsb,st.johnsonsu,st.ksone,st.kstwobign,st.laplace,st.levy,st.levy_l,st.levy_stable,
    st.logistic,st.loggamma,st.loglaplace,st.lognorm,st.lomax,st.maxwell,st.mielke,st.nakagami,st.ncx2,st.ncf,
    st.nct,st.norm,st.pareto,st.pearson3,st.powerlaw,st.powerlognorm,st.powernorm,st.rdist,st.reciprocal,
    st.rayleigh,st.rice,st.recipinvgauss,st.semicircular,st.t,st.triang,st.truncexpon,st.truncnorm,st.tukeylambda,
    st.uniform,st.vonmises,st.vonmises_line,st.wald,st.weibull_min,st.weibull_max,st.wrapcauchy,
    st.bernoulli, st.binom, st.boltzmann, st.dlaplace, st.geom, st.hypergeom, 
    st.logser, st.nbinom, st.planck, st.poisson, st.randint, st.skellam, st.zipf
]

class CheckForBestFit():
  # Create models from data
  def best_fit_distribution(self, data):
    # Best holders
    best_ks = []

    # Estimate distribution parameters from data
    for distribution in DISTRIBUTIONS:
      # Try to fit the distribution
      try:
        # Ignore warnings from data that can't be fit
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore')

            # fit dist to data
            params = distribution.fit(data)

            # Separate parts of parameters
            arg = params[:-2]
            loc = params[-2]
            scale = params[-1]

            if len(params)==3:
              model_data = distribution.rvs(arg, loc=loc, scale=scale, size=len(data))
            elif len(params)==4:
              model_data = distribution.rvs(params[0], params[1], loc=loc, scale=scale, size=len(data))
            elif len(params)==5:
              model_data = distribution.rvs(params[0], params[1], params[2], loc=loc, scale=scale, size=len(data))
            elif len(params)==6:
              model_data = distribution.rvs(params[0], params[1], params[2], params[3], loc=loc, scale=scale, size=len(data))
            else:
              model_data = distribution.rvs(loc=loc, scale=scale, size=len(data))
            ks_stats, ks_p = st.ks_2samp(data, model_data)
            
            ks_t_s, ks_t_p = st.kstest(x, distribution.name, args=params)
#             print(distribution.name, ks_stats, ks_p, )
            best_ks.append([ks_stats, ks_p, distribution, ks_t_p])

      except Exception:
        pass
    
    best_t_p = [0,0]
    best_p = [0,0]
    best_s = [0,0]
    best_d = [st.norm, st.norm]
    
    for s,p,d,t_p in best_ks:
      if p > best_p[0]:
        best_t_p[0] = t_p
        best_p[0] = p
        best_s[0] = s
        best_d[0] = d
      
      if t_p > best_t_p[1]:
        best_t_p[1] = t_p
        best_p[1] = p
        best_s[1] = s
        best_d[1] = d
    
    print("BEST by sample:",best_d[0].name, best_s[0],best_p[0],best_t_p[0])
    print("BEST by continuous:",best_d[1].name, best_s[1],best_p[1],best_t_p[1])

  def __init__(self, data_to_load):
    data = pd.Series(data_to_load)
    self.best_fit_distribution(data)

    


if use_theoretical_models == True:
  print("Bug creation:")    
  CheckForBestFit(global_created_bugs)

  print("\nBug fixation:") 
  CheckForBestFit(global_fixed_bugs)

#@title Best fit function

# # Distributions to check
# DISTRIBUTIONS = [        
#     st.alpha,st.anglit,st.arcsine,st.beta,st.betaprime,st.bradford,st.burr,st.cauchy,st.chi,st.chi2,st.cosine,
#     st.dgamma,st.dweibull,st.erlang,st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.fatiguelife,st.fisk,
#     st.foldcauchy,st.foldnorm,st.frechet_r,st.frechet_l,st.genlogistic,st.genpareto,st.gennorm,st.genexpon,
#     st.genextreme,st.gausshyper,st.gamma,st.gengamma,st.genhalflogistic,st.gilbrat,st.gompertz,st.gumbel_r,
#     st.gumbel_l,st.halfcauchy,st.halflogistic,st.halfnorm,st.halfgennorm,st.hypsecant,st.invgamma,st.invgauss,
#     st.invweibull,st.johnsonsb,st.johnsonsu,st.ksone,st.kstwobign,st.laplace,st.levy,st.levy_l,st.levy_stable,
#     st.logistic,st.loggamma,st.loglaplace,st.lognorm,st.lomax,st.maxwell,st.mielke,st.nakagami,st.ncx2,st.ncf,
#     st.nct,st.norm,st.pareto,st.pearson3,st.powerlaw,st.powerlognorm,st.powernorm,st.rdist,st.reciprocal,
#     st.rayleigh,st.rice,st.recipinvgauss,st.semicircular,st.t,st.triang,st.truncexpon,st.truncnorm,st.tukeylambda,
#     st.uniform,st.vonmises,st.vonmises_line,st.wald,st.weibull_min,st.weibull_max,st.wrapcauchy
# ]

# DISTRIBUTIONS = [        
#     st.bernoulli, st.binom, st.boltzmann, st.dlaplace, st.geom, st.hypergeom, 
#     st.logser, st.nbinom, st.planck, st.poisson, st.randint, st.skellam, st.zipf
# ]

class FindBestFit():
#     def check_chisquare(self, p, degrees_of_freedom):
#         return st.chi2.cdf(p, degrees_of_freedom)
    
    # Create models from data
    def best_fit_distribution(self, data, bins=200, ax=None):
        """Model data by finding best fit distribution to data"""
        # Get histogram of original data
        y, x = np.histogram(data, bins=bins, density=True)
        x = (x + np.roll(x, -1))[:-1] / 2.0

        # Best holders
        best_distribution = st.norm
        best_params = (0.0, 1.0)
        best_sse = np.inf
        best_chisquare = 0
        
        # Estimate distribution parameters from data
        for distribution in DISTRIBUTIONS:

            # Try to fit the distribution
            try:
                # Ignore warnings from data that can't be fit
                with warnings.catch_warnings():
                    warnings.filterwarnings('ignore')

                    # fit dist to data
                    params = distribution.fit(data)

                    # Separate parts of parameters
                    arg = params[:-2]
                    loc = params[-2]
                    scale = params[-1]

                    # Calculate fitted PDF and error with fit in distribution
                    pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)
                    sse = np.sum(np.power(y - pdf, 2.0))
#                     chisquare = np.sum(np.power(y - pdf, 2.0) / pdf)

                    # if axis pass in add to plot
                    try:
                        if ax:
                            pd.Series(pdf, x).plot(ax=ax)
                        end
                    except Exception:
                        pass
                    
#                     print(distribution.name, st.ks_2samp(y, pdf), sse)
#                     print(distribution.name, sse, self.check_chisquare(chisquare, bins-1))
                    # identify if this distribution is better
                    if best_sse > sse > 0:
                        best_distribution = distribution
                        best_params = params
                        best_sse = sse
#                         best_chisquare = chisquare
                        

            except Exception:
                pass

#         print("BEST:",best_distribution.name, best_params, best_sse)
        print("Total error:", best_sse)
        return best_distribution, best_params

    def make_pdf(self, dist, params, size=10000):
        """Generate distributions's Probability Distribution Function """

        # Separate parts of parameters
        arg = params[:-2]
        loc = params[-2]
        scale = params[-1]

        # Get sane start and end points of distribution
        start = dist.ppf(0.01, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.01, loc=loc, scale=scale)
        end = dist.ppf(0.99, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.99, loc=loc, scale=scale)

        # Build PDF and turn into pandas Series
        x = np.linspace(start, end, size)
        y = dist.pdf(x, loc=loc, scale=scale, *arg)
        pdf = pd.Series(y, x)

        return pdf
      
    def __init__(self, data_to_load):
#         matplotlib.rcParams['figure.figsize'] = (16.0, 12.0)
#         matplotlib.style.use('ggplot')
        plot_size = 3
      
        # Load data from statsmodels datasets
        # data = pd.Series(sm.datasets.elnino.load_pandas().data.set_index('YEAR').values.ravel())
        data = pd.Series(data_to_load)

        # Plot for comparison
        plt.figure(figsize=(3*plot_size,2*plot_size))
        ax = data.plot(kind='hist', bins=200, density=True, alpha=0.5)
        # Save plot limits
        dataYLim = ax.get_ylim()
        # Find best fit distribution
        self.best_fit, self.best_fit_params = self.best_fit_distribution(data, max(data), ax)
        best_dist = getattr(st, self.best_fit.name)

        # Update plots
        ax.set_ylim(dataYLim)
        ax.set_title(u'All Fitted Distributions')
        ax.set_xlabel(u'Days')
        ax.set_ylabel('Frequency')
        plt.show()

        # Make PDF with best params 
        pdf = self.make_pdf(best_dist, self.best_fit_params)

        # Display
        plt.figure(figsize=(3*plot_size,2*plot_size))
        ax = pdf.plot(lw=2, label='PDF', legend=True)
        data.plot(kind='hist', bins=200, density=True, alpha=0.5, label='Data', legend=True, ax=ax)

        param_names = (best_dist.shapes + ', loc, scale').split(', ') if best_dist.shapes else ['loc', 'scale']
        param_str = ', '.join(['{}={:0.2f}'.format(k,v) for k,v in zip(param_names, self.best_fit_params)])
        dist_str = '{}({})'.format(self.best_fit.name, param_str)

        ax.set_title(u'Best fit distribution: \n' + dist_str)
        ax.set_xlabel(u'Days')
        ax.set_ylabel('Frequency')
        plt.show()
        
    def return_best_fit(self):
        return self.best_fit, self.best_fit_params


if use_theoretical_models == True:
  #@markdown Finding best fit for bug creation
  find_best_fit = FindBestFit(global_created_bugs)
  bug_creation_best_fit, bug_creation_bf_params = find_best_fit.return_best_fit()

  #@markdown Finding best fit for bug fixation
  find_best_fit = FindBestFit(global_fixed_bugs)
  bug_fixation_best_fit, bug_fixation_bf_params = find_best_fit.return_best_fit()

#@title Distributions for ECDF
def generate_cdf_for_ecdf(data):
  ecdf = ECDF(data)
  
  cdf_list = []
  if use_float_days == True:
    for i in data:
      cdf_list.append(ecdf([i]))
  else: 
    for i in range(max(data)+1):
      cdf_list.append(ecdf([i]))
  
  return cdf_list

ecdf_probablity_distribution_bc = []
ecdf_probablity_distribution_bf = []
ecdf_probablity_distribution_br = []

# saving a copy
global_created_bugs_sorted = global_created_bugs.copy()
global_created_bugs_sorted.sort()
global_fixed_bugs_sorted = global_fixed_bugs.copy()
global_fixed_bugs_sorted.sort()
global_resolution_time_sorted = global_resolution_time.copy()
global_resolution_time_sorted.sort()

# fix vs kludge
ecdf_of_fixes = []
ecdf_of_kludges = []
global_fixes_periods_sorted = sorted(global_fixes_periods)
global_kludges_periods_sorted = sorted(global_kludges_periods)

if use_theoretical_models == False: 
  if use_float_days == True:
    ecdf_probablity_distribution_bc = generate_cdf_for_ecdf(global_created_bugs_sorted)
    ecdf_probablity_distribution_bf = generate_cdf_for_ecdf(global_fixed_bugs_sorted)
    ecdf_probablity_distribution_br = generate_cdf_for_ecdf(global_resolution_time_sorted)
    ecdf_of_fixes = generate_cdf_for_ecdf(global_fixes_periods_sorted)
    ecdf_of_kludges = generate_cdf_for_ecdf(global_kludges_periods_sorted)
  else:
    ecdf_probablity_distribution_bc = generate_cdf_for_ecdf(global_created_bugs)
    ecdf_probablity_distribution_bf = generate_cdf_for_ecdf(global_fixed_bugs)
    ecdf_probablity_distribution_br = generate_cdf_for_ecdf(global_resolution_time)
  
def ecdf_random_number(data):
  rand = random.uniform(0, 1)
  # get the position from bisecting
  pos = bisect_left(data, rand)
#   print(rand, pos)

  # manually first and last to avoid exception error further on
  if pos == 0 or pos == len(data): return pos

  # return the closest between two
  before = data[pos-1]
  after = data[pos]
  if after - rand < rand - before: return pos # after
  else: return pos-1 # before

def ecdf_random_number_(distr, data):
  rand = random.uniform(0, 1)
  # get the position from bisecting
  pos = bisect_left(distr, rand)
#   print(rand, pos)

  # manually first and last to avoid exception error further on
  if pos == 0 or pos == len(distr): return data[pos]

  # return the closest between two
  before = distr[pos-1]
  after = distr[pos]
  if after - rand < rand - before: return data[pos] # after
  else: return data[pos-1] # before

# av_temp = 0
# for i in range(1000000):
#   av_temp += ecdf_random_number_(ecdf_probablity_distribution_bc, global_created_bugs_sorted)
  
# print(av_temp / 1000000)

#@markdown Playing with the code

# from rpy2.robjects import r
# from rpy2.robjects import pandas2ri
# from rpy2.robjects.packages import importr

# MASS = importr('MASS')
# pandas2ri.activate()

# # df_temp = pd.DataFrame()
# # df_temp["Values"] = global_created_bugs
# # ri_temp = pandas2ri.py2ri(df_temp)

# my_array = np.asarray(global_created_bugs)
# params_temp = MASS.fitdistr(my_array, 'normal')
# print(params_temp)




# class Distribution(object):
    
#     def __init__(self,dist_names_list = []):
#         self.dist_names = ['norm','lognorm','expon','pareto'] #['bernoulli']
#         self.dist_results = []
#         self.params = {}
        
#         self.DistributionName = ""
#         self.PValue = 0
#         self.Param = None
        
#         self.isFitted = False
        
#     def Fit(self, y):
#         self.dist_results = []
#         self.params = {}
#         for dist_name in self.dist_names:
#             dist = getattr(scipy.stats, dist_name)
#             param = dist.fit(y)
            
#             self.params[dist_name] = param
#             #Applying the Kolmogorov-Smirnov test
#             D, p = scipy.stats.kstest(y, dist_name, args=param);
#             self.dist_results.append((dist_name,p))
#             print(dist_name, p)

#         #select the best fitted distribution
#         sel_dist,p = (max(self.dist_results,key=lambda item:item[1]))
#         #store the name of the best fit and its p value
#         self.DistributionName = sel_dist
#         self.PValue = p
        
#         self.isFitted = True
#         return self.DistributionName,self.PValue

# print(global_created_bugs)
# distribution = Distribution()
# distribution.Fit(global_created_bugs)




# rnorm = r('rnorm')  # create an alias to the R function rnorm
# x = rnorm(n=1000, mean=0, sd=1) # generates the data sample (normal distribution)

# params = MASS.fitdistr(x, 'normal')  # perform the fitting here
# print(params)




# x_temp_temp = np.array([37.50,46.79,48.30,46.04,43.40,39.25])
# # MASS.fitdistr(x_temp_temp, 'weibull')
# x_temp_temp




# df_temp = pd.DataFrame()
# df_temp["Values"] = [37.50,46.79,48.30,46.04,43.40,39.25]
# df_temp




# # pandas2ri.activate()
# # pd_df = pandas2ri.ri2py_dataframe(rdf) # pandas2ri.ri2py()
# ri_temp = pandas2ri.py2ri(df_temp)
# ri_temp




# temp_list = [37.50,46.79,48.30,46.04,43.40,39.25]
# class(temp_list)="numeric" 
# pandas2ri.activate()

# my_list = [37.50,46.79,48.30,46.04,43.40,39.25]
# my_array = np.asarray(my_list)
# params_temp = MASS.fitdistr(my_array, 'normal')
# print(params_temp)




# params_temp = MASS.fitdistr(my_array, 'weibull')
# print(params_temp)




# MASS.fitdistr(my_array, 'weibull')




# MASS.descdist(my_array)




# rscript = """
# mydata <- c(37.50,46.79,48.30,46.04,43.40,39.25)
# fitdistr(mydata, "weibull")
# """




# expression = r.parse(rscript)




# !pip install pyensae
# !pip install antlr4-python3-runtime




# from pyensae.languages import r2python
# print(r2python(rscript, pep8=True))
# r2python(rscript, pep8=True)




# #not including the lines of code that create the pandas dataframe #data_pd_df
# #convert pandas dataframe to R dataframe
# data_r_df = com.convert_to_r_dataframe(data_pd_df)
# #importing R package custom.analytics and replacing . with _ in #package names to ensure no conflicts
# d = {'package.dependencies': 'package_dot_dependencies',
#      'package_dependencies': 'package_uscore_dependencies'}
# custom_analytics = importr('custom.analytics', 
#                    robject_translations = d)
# #calling functions from our R package custom.analytics in Python to #generate results
# stage1_processed_data = custom_analytics.process_filetype1(data_r_df)
# summary_stats = custom_analytics.summarize_filetype1(stage1_processed_data)

#@markdown Playing with the code 2


# # simple example of theoretical models not working
# params = st.halfgennorm.fit(global_created_bugs)
# model_data = st.halfgennorm.rvs(params[0], params[-1], size=len(global_created_bugs))
# new_model_data = [int(item) for item in model_data]
# print(params)
# print(new_model_data)
# print(len(new_model_data), len(global_created_bugs))
# print(st.ks_2samp(global_created_bugs, new_model_data))







# # def check_chisquare(data, pdf):
# #   chisquare = np.sum(np.power(data - pdf, 2.0) / pdf)
# #   return st.chi2.cdf(chisquare, len(data)-1)
# # def theoretical_distr(data, distr, params):
# #   pdf = distr.rvs(params[0], loc=params[1], scale=params[2], 
# #                             size=len(data))
# #   return pdf

# print("Bug creation:")
# # theoretical_distr_bc = theoretical_distr(global_created_bugs, bug_creation_best_fit, bug_creation_bf_params)
# # print(bug_creation_best_fit.name, bug_creation_best_fit, bug_creation_bf_params)
# # print("Similarity with theoretical model:", check_chisquare(global_created_bugs, theoretical_distr_bc))

# x = np.asarray(global_created_bugs)
# y = bug_creation_best_fit.rvs(bug_creation_bf_params[0], 
#                               loc=bug_creation_bf_params[1], 
#                               scale=bug_creation_bf_params[2], 
#                               size=len(global_created_bugs))
# print(st.ks_2samp(x, y))

# # print("\nBug fixation:")
# # theoretical_distr_bf = theoretical_distr(global_fixed_bugs, bug_fixation_best_fit, bug_fixation_bf_params)
# # print(bug_fixation_best_fit.name, bug_fixation_best_fit, bug_fixation_bf_params)
# # print("Similarity with theoretical model:", check_chisquare(global_fixed_bugs, theoretical_distr_bf))

# # x = np.asarray(global_fixed_bugs)
# # vals = bug_fixation_best_fit.ppf(x, bug_fixation_bf_params[0])
# # y = bug_fixation_best_fit.cdf(vals, 
# #                               bug_fixation_bf_params[0], 
# #                               loc=bug_fixation_bf_params[1], 
# #                               scale=bug_fixation_bf_params[2])
# # print(st.ks_2samp(x, y))

# # x = np.asarray(global_fixed_bugs)
# # y = st.gennorm.rvs(bug_fixation_bf_params, size=len(global_fixed_bugs))
# # print(st.ks_2samp(x, y))



# # np.asarray(global_fixed_bugs)
# # print("Chi Square:", best_chisquare, "Degrees of Freedom:", bins-1, # or len(y)
# #   "Chance Probability:", self.check_chisquare(best_chisquare, bins-1))
# # print(st.kstest(data, 'halfgennorm'))





# x = np.array([2,3,4])
# y = np.array([4,3])
# print(st.ks_2samp(x, y))





# param_t = st.halfgennorm.fit(global_fixed_bugs)
# # x = np.asarray(global_created_bugs)
# # y = st.alpha.rvs(args=param_t, 
# #                  size=len(global_created_bugs))
# print(st.kstest(x, 'halfgennorm', args=param_t))




# st.chi2.fit(global_fixed_bugs)




# class Distribution(object):
    
#     def __init__(self,dist_names_list = []):
# #         self.dist_names = dist_names = [ 'alpha', 'anglit', 'arcsine', 'beta', 'betaprime', 'bradford', 'burr', 'cauchy', 'chi', 'chi2', 'cosine', 'dgamma', 'dweibull', 'erlang', 'expon', 'exponweib', 'exponpow', 'f', 'fatiguelife', 'fisk', 'foldcauchy', 'foldnorm', 'frechet_r', 'frechet_l', 'genlogistic', 'genpareto', 'genexpon', 'genextreme', 'gausshyper', 'gamma', 'gengamma', 'genhalflogistic', 'gilbrat', 'gompertz', 'gumbel_r', 'gumbel_l', 'halfcauchy', 'halflogistic', 'halfnorm', 'hypsecant', 'invgamma', 'invgauss', 'invweibull', 'johnsonsb', 'johnsonsu', 'ksone', 'kstwobign', 'laplace', 'logistic', 'loggamma', 'loglaplace', 'lognorm', 'lomax', 'maxwell', 'mielke', 'nakagami', 'ncx2', 'ncf', 'nct', 'norm', 'pareto', 'pearson3', 'powerlaw', 'powerlognorm', 'powernorm', 'rdist', 'reciprocal', 'rayleigh', 'rice', 'recipinvgauss', 'semicircular', 't', 'triang', 'truncexpon', 'truncnorm', 'tukeylambda', 'uniform', 'vonmises', 'wald', 'weibull_min', 'weibull_max', 'wrapcauchy'] 
#         self.dist_names = [distr.name for distr in DISTRIBUTIONS]
#         self.dist_results = []
#         self.params = {}
        
#         self.DistributionName = ""
#         self.PValue = 0
#         self.Param = None
        
#         self.isFitted = False
        
        
#     def fit(self, y):
#         self.dist_results = []
#         self.params = {}
#         for dist_name in self.dist_names:
#             dist = getattr(scipy.stats, dist_name)
#             param = dist.fit(y)
            
#             self.params[dist_name] = param
#             #Applying the Kolmogorov-Smirnov test
#             D, p = scipy.stats.kstest(y, dist_name, args=param);
#             self.dist_results.append((dist_name,p))
#             print(p)

#         #select the best fitted distribution
#         sel_dist,p = (max(self.dist_results,key=lambda item:item[1]))
#         #store the name of the best fit and its p value
#         self.DistributionName = sel_dist
#         self.PValue = p
        
#         self.isFitted = True
#         return self.DistributionName,self.PValue
    
#     def random(self, n = 1):
#         if self.isFitted:
#             dist_name = self.DistributionName
#             param = self.params[dist_name]
#             #initiate the scipy distribution
#             dist = getattr(scipy.stats, dist_name)
#             return dist.rvs(*param[:-2], loc=param[-2], scale=param[-1], size=n)
#         else:
#             raise ValueError('Must first run the Fit method.')
            
#     def plot(self,y):
#         x = self.Random(n=len(y))
#         plt.hist(x, alpha=0.5, label='Fitted')
#         plt.hist(y, alpha=0.5, label='Actual')
#         plt.legend(loc='upper right')
        
# distribution = Distribution().fit(global_fixed_bugs)

"""## Running the simulation

### Creation vs Fixation
"""

#@markdown ### Enter how many days to simulate
how_many_days = 1000 #@param {type:"integer"}

#@title Simulation of "creation_vs_fixation"

day_of_simulation = []
total_bugs_of_simulation = []

class BugSimulation(object):
  def __init__(self, env):
    self.total_bugs = 0
    
    self.env = env
    self.env.process(self.bug_creation())
    self.env.process(self.bug_fixation())
    
    self.save_info()
  
  def bug_creation(self):
    while True:
      bug_creation_time = 0
      if use_theoretical_models == True:
        bug_creation_time = int(bug_creation_best_fit.rvs(bug_creation_bf_params[0], 
                                                        loc=bug_creation_bf_params[1], 
                                                        scale=bug_creation_bf_params[2], 
                                                        size=1)) #2 # <- normal distribution
      else:
        if use_float_days == True:
          bug_creation_time = ecdf_random_number_(ecdf_probablity_distribution_bc, global_created_bugs_sorted)
        else:
          bug_creation_time = ecdf_random_number(ecdf_probablity_distribution_bc)
      
#       print("C",bug_creation_time)
      yield self.env.timeout(bug_creation_time)
      
      self.total_bugs += 1
      self.save_info()

  def bug_fixation(self):
    while True:
      if self.total_bugs > 0:
        bug_fixation_time = 0
        if use_theoretical_models == True:
          bug_fixation_time = int(bug_fixation_best_fit.rvs(bug_fixation_bf_params[0], 
                                                            loc=bug_fixation_bf_params[1], 
                                                            scale=bug_fixation_bf_params[2], 
                                                            size=1)) #3 # <- pareto distribution
        
        else:
          if use_float_days == True:
            bug_fixation_time = ecdf_random_number_(ecdf_probablity_distribution_bf, global_fixed_bugs_sorted)
          else:
            bug_fixation_time = ecdf_random_number(ecdf_probablity_distribution_bf)
        
#         print("F",bug_fixation_time)
        yield self.env.timeout(bug_fixation_time)

        self.total_bugs -= 1
        self.save_info()
      
      else: # nothing to fix
        yield self.env.timeout(1)
        
  
  def save_info(self):
    day_of_simulation.append(self.env.now)
    total_bugs_of_simulation.append(self.total_bugs)

# if use_float_days == True:
#   print("Note, this model does not support float days. Information may be inaccurate.")
    
env = simpy.Environment()
BugSimulation(env)
env.run(until=how_many_days)

#@title Show results of the simulation

# print(day_of_simulation)
# print(total_bugs_of_simulation,"\n")

print_stats(total_bugs_of_simulation)
plot_two_lists('Days', day_of_simulation, 'Total bugs', 
               total_bugs_of_simulation, 'line', 
               10)

"""### Creation vs Resolution"""

#@markdown ### Enter how many days to simulate
how_many_days = 10000 #@param {type:"integer"}

#@markdown ### Enter the number of active developers
how_many_developers = 100 #@param {type:"integer"}

# @title Simulation of "creation_vs_resolution"

day_of_simulation = []
total_bugs_of_simulation = []

class BugSimulation(object):
  def __init__(self, env, bcs):
    self.total_bugs = 0
    self.active_devs = 0
    
    self.total_bugs_fixed = 0
    self.days_to_create = []
    self.days_to_resolve = []
    
    self.env = env
    self.bcs = bcs
    
    self.env.process(self.bug_creation())
    self.save_info()
  
  def bug_creation(self):
    while True:
      # create bug creation time to wait
      bug_creation_time = ecdf_random_number_(ecdf_probablity_distribution_bc, global_created_bugs_sorted)
#       print("C",bug_creation_time, "at", self.env.now)
      self.days_to_create.append(bug_creation_time)
      yield self.env.timeout(bug_creation_time)
      
      # save information
      self.total_bugs += 1
      self.save_info()
      
      # launch a developer
      self.env.process(self.bug_resolution())
    
    
  def bug_resolution(self):
    with self.bcs.request() as req:
      yield req
      
      self.active_devs += 1
      bug_resolution_time = ecdf_random_number_(ecdf_probablity_distribution_br, global_resolution_time_sorted)
#       print("F", bug_resolution_time, "D", self.active_devs, "at", self.env.now)
      self.days_to_resolve.append(bug_resolution_time)
      yield self.env.timeout(bug_resolution_time)

      self.total_bugs -= 1
      self.active_devs -= 1
      self.total_bugs_fixed += 1
      
      self.save_info()

  def save_info(self):
    day_of_simulation.append(self.env.now)
    total_bugs_of_simulation.append(self.total_bugs)
  
  def return_info(self):
    return self.total_bugs_fixed, self.days_to_create, self.days_to_resolve
    

if use_theoretical_models == True:
  print("Doesn't support theoretical models.")

if use_float_days == True:
  env = simpy.Environment()
  bcs = simpy.Resource(env, capacity=how_many_developers)
  bug_simulation = BugSimulation(env, bcs)
  env.run(until=how_many_days)
  
  t, c, r = bug_simulation.return_info()
  print("Total bugs fixed:", t, "\n")
  
  print("* Data creation stats:")
  print_stats(c)
  
  print("\n* Data resolution stats:")
  print_stats(r)
  
else:
  print("Doesn't support discrete data.")

#@title Show results of the simulation
# print(day_of_simulation)
# print(total_bugs_of_simulation,"\n")

print_stats(total_bugs_of_simulation)
plot_two_lists('Days', day_of_simulation, 'Total bugs', 
               total_bugs_of_simulation, 'line', 
               10)

#@markdown Playing with the code

# # np.random.choice(np.arange(1, 7), p=[0.1, 0.05, 0.05, 0.2, 0.4, 0.2])

# # rvs(c, loc=0, scale=1, size=1, random_state=None)
# value = st.genpareto.rvs(6.31, loc=0, scale=2.95, size=1)
# print(value, int(value))

# # rvs(beta, loc=0, scale=1, size=1, random_state=None)
# value = st.halfgennorm.rvs(0.34, loc=0, scale=0.02, size=1)
# print(value, int(value))




# def car(env):
#   while True:
#     print('Start parking at %d' % env.now)
#     parking_duration = 5
#     yield env.timeout(parking_duration)

#     print('Start driving at %d' % env.now)
#     trip_duration = 2
#     yield env.timeout(trip_duration)

# env = simpy.Environment()
# env.process(car(env))
# env.run(until=15)




# class Car(object):
#   def __init__(self, env):
#       self.env = env
      
#       self.action = env.process(self.run())

#   def run(self):
#       while True:
#           print('Start parking and charging at %d' % self.env.now)
#           charge_duration = 5
#           # We may get interrupted while charging the battery
#           try:
#               yield self.env.process(self.charge(charge_duration))
#           except simpy.Interrupt:
#               # When we received an interrupt, we stop charging and
#               # switch to the "driving" state
#               print('Was interrupted. Hope, the battery is full enough ...')

#           print('Start driving at %d' % self.env.now)
#           trip_duration = 2
#           yield self.env.timeout(trip_duration)

#   def charge(self, duration):
#       yield self.env.timeout(duration)

# def driver(env, car):
#     yield env.timeout(3)
#     car.action.interrupt()
      
# env = simpy.Environment()
# car = Car(env)

# env.process(driver(env, car))
# env.run(until=25)




# # Arrival rate and length of stay inputs.
# ARR_RATE = 0.4
# MEAN_LOS_OBS = 3
# MEAN_LOS_LDR = 12
# MEAN_LOS_PP = 48

# RNG_SEED = 6354

# def source(env, arr_rate, prng=RandomState(0)):
#     patients_created = 0

#     # Infinite loop for generating patients according to a poisson process.
#     while True:

#         # Generate next interarrival time
#         iat = prng.exponential(1.0 / arr_rate)

#         # Generate length of stay in each unit for this patient
#         los_obs = prng.exponential(MEAN_LOS_OBS)
#         los_ldr = prng.exponential(MEAN_LOS_LDR)
#         los_pp = prng.exponential(MEAN_LOS_PP)

#         # Update counter of patients
#         patients_created += 1

#         # Create a new patient flow process.
#         obp = obpatient_flow(env, 'Patient {}'.format(patients_created),
#                              los_obs=los_obs, los_ldr=los_ldr, los_pp=los_pp)

#         # Register the process with the simulation environment
#         env.process(obp)

#         # This process will now yield to a 'timeout' event. This process will resume after iat time units.
#         yield env.timeout(iat)

# # Define an obpatient_flow "process"
# def obpatient_flow(env, name, los_obs, los_ldr, los_pp):

#     # Note how we are simply modeling each stay as a delay. There
#     # is NO contention for any resources.
#     print("{} entering OBS at {:.4f}".format(name, env.now))
#     yield env.timeout(los_obs)

#     print("{} entering LDR at {:.4f}".format(name, env.now))
#     yield env.timeout(los_ldr)

#     print("{} entering PP at {:.4f}".format(name, env.now))
#     yield env.timeout(los_pp)


# # Initialize a simulation environment
# env = simpy.Environment()

# # Initialize a random number generator.
# # See https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.RandomState.html
# prng = RandomState(RNG_SEED)

# # Create a process generator and start it and add it to the env
# # Calling obpatient(env) creates the generator.
# # env.process() starts and adds it to env
# runtime = 25
# env.process(source(env, ARR_RATE, prng))

# # Run the simulation
# env.run(until=runtime)

"""### Creation vs Resolution 2: Fixes vs Kludges
\+ "REOPEN" is introduced
"""

#@markdown ### Enter how many days to simulate
how_many_days = 365 #@param {type:"integer"}

#@markdown ### Enter the number of active developers
how_many_developers = 30 #@param {type:"slider", min:1, max:100, step:1}

all_developers_list = developers.return_devs()
if how_many_developers > len(all_developers_list):
  print("Too many developers. Simulation will fail. This number exceeds the maximum number of real developers, which is:", len(developers.return_devs()))

#@title All developers printed

# sort_all_devs = all_developers_list.copy()
# all_developers_list.sort(key=lambda x: x[1], reverse=True) # true for worst
# all_developers_list = sort_all_devs
all_developers_list

# @title Simulation of "creation_vs_resolution" v2

day_of_simulation = []
total_bugs_of_simulation = []

class BugSimulation_two(object):
  def __init__(self, env, bcs, dev_ids = [i for i in range(how_many_developers)]):
#     self.dev_ids = [i for i in range(how_many_developers)]
    self.dev_ids = dev_ids
    
    self.total_bugs_queued = 0
    self.active_devs = 0
    
    self.total_bugs_fixed = 0
    self.total_bugs_reported = 0
    self.total_reopened = 0
    self.total_fix = 0
    self.total_kludge = 0
    
    self.days_to_create = []
    self.days_to_resolve_f = []
    self.days_to_resolve_k = []
    
    self.env = env
    self.bcs = bcs
    
    self.env.process(self.bug_creation())
    self.save_info()
  
  def bug_creation(self):
    while True:
      # create bug creation time to wait
      bug_creation_time = ecdf_random_number_(ecdf_probablity_distribution_bc, global_created_bugs_sorted)
#       print("C",bug_creation_time, "at", self.env.now)
      self.days_to_create.append(bug_creation_time)
      yield self.env.timeout(bug_creation_time)
      
      # save information
      self.total_bugs_queued += 1
      self.total_bugs_reported += 1
      self.save_info()
      
      # launch a developer
      self.env.process(self.bug_resolution())
    
    
  def bug_resolution(self):
    with self.bcs.request() as req:
      yield req
      
      self.active_devs += 1
      bug_resolution_time, dev_id = self.pick_dev()
#       print("F", bug_resolution_time, "D", self.active_devs, "at", self.env.now)
      yield self.env.timeout(bug_resolution_time)

      self.release_dev(dev_id)
      self.total_bugs_queued -= 1
      self.active_devs -= 1
      self.total_bugs_fixed += 1
      
      self.save_info()
      
  def pick_dev(self):
    # picking a developer randomly
    dev_id = random.choice(self.dev_ids)
    self.dev_ids.remove(dev_id) # remove from the list of available developers
#     print("\nDEV:", dev_id)
    
    time = self.fix_vs_kludge(dev_id)
#     print(time, "at", self.env.now)
    return time, dev_id
      
  def release_dev(self, dev_id):
    self.dev_ids.append(dev_id)
      
  
  def fix_vs_kludge(self, dev_id):
    # ecdf_of_fixes = global_fixes_period
    # ecdf_of_kludges = global_kludges_periods  
    # get either a kludge or a fix
    time = 0
    rand = random.uniform(0, 1)    
    if rand > all_developers_list[dev_id][1]: # less means a kludge
#       print("FIX")
      time = ecdf_random_number_(ecdf_of_fixes, global_fixes_periods_sorted)
      self.days_to_resolve_f.append(time) # for fix
      self.total_fix += 1
    else:
#       print("KLUDGE")
      time = ecdf_random_number_(ecdf_of_kludges, global_kludges_periods_sorted)
      self.days_to_resolve_k.append(time) # for kludge
      self.total_kludge += 1
      
    rand = random.uniform(0, 1)
    if rand < global_chance_of_reopen: # less means a reopen
#       print(" - Reopened for developer:", dev_id)
      self.total_reopened += 1
      self.fix_vs_kludge(dev_id)
    
    return time

  
  def save_info(self):
    day_of_simulation.append(self.env.now)
    total_bugs_of_simulation.append(self.total_bugs_queued)
  
  def return_info(self):
    return self.total_bugs_fixed, self.total_bugs_reported, self.total_reopened, self.total_fix, self.total_kludge
  
  def return_stats(self):
    return self.days_to_create, self.days_to_resolve_f, self.days_to_resolve_k
  

if use_theoretical_models == True:
  print("Doesn't support theoretical models.")

elif use_float_days == True:
  env = simpy.Environment()
  bcs = simpy.Resource(env, capacity=how_many_developers)
  bug_simulation = BugSimulation_two(env, bcs)
  env.run(until=how_many_days)
  
  t, r, o, f, k = bug_simulation.return_info()
  print("\nTotal bugs resolved:", t)
  print("Total bugs reported:", r)
  print("Total times reopened:", o)
  print("Total bugs-fixes:", f)
  print("Total bugs-kludges:", k, "\n")
  
  
  c, rf, rk = bug_simulation.return_stats()
  print("* Data creation stats:")
  print_stats(c)
  
  print("\n* Data resolution stats (fix):")
  print_stats(rf)
  
  print("\n* Data resolution stats (kludge):")
  print_stats(rk)
  
else:
  print("Doesn't support discrete data.")

#@title Show results of the simulation
# print(day_of_simulation)
# print(total_bugs_of_simulation,"\n")

print_stats(total_bugs_of_simulation)
plot_two_lists('Days', day_of_simulation, 'Total bugs', 
               total_bugs_of_simulation, 'line', 
               10)

#@markdown ### How many time to run the simulation?
how_many_times = 1000 #@param {type:"integer"}

if how_many_times < 0:
  print("Must be bigger than 0.")
  
#@markdown ### And for how many days?
how_many_days = 365 #@param {type:"integer"}

#@markdown ### Enter the number of active developers
how_many_developers = 10 #@param {type:"slider", min:1, max:100, step:1}

all_developers_list = developers.return_devs()
if how_many_developers > len(all_developers_list):
  print("Too many developers. Simulation will fail. This number exceeds the maximum number of real developers, which is:", len(developers.return_devs()))

#@title Running the same simulation many times
day_of_simulation = []
total_bugs_of_simulation = []

if use_theoretical_models == True:
  print("Doesn't support theoretical models.")

elif use_float_days == True:
  # saving total stats
  total_t, total_r, total_o, total_f, total_k = 0, 0, 0, 0, 0
  
  for i in range(how_many_times):
    env = simpy.Environment()
    bcs = simpy.Resource(env, capacity=how_many_developers)
    bug_simulation = BugSimulation_two(env, bcs, [i for i in range(how_many_developers)])
    env.run(until=how_many_days)
  
    t, r, o, f, k = bug_simulation.return_info()
    
    total_t += t
    total_r += r
    total_o += o
    total_f += f
    total_k += k
    
  print("Average number of bugs resolved: %.2f" % (total_t / how_many_times))
  print("Average number of bugs reported: %.2f" % (total_r / how_many_times))
  print("Average number of times reopened: %.2f" % (total_o / how_many_times))
  print("Average number of bugs-fixes: %.2f" % (total_f / how_many_times))
  print("Average number of bugs-kludges: %.2f" % (total_k / how_many_times))
  
else:
  print("Doesn't support discrete data.")

"""## Scenario Modelling"""

# run this for scenario modelling of all developers
all_all_devs = developers.return_full_dev_stats()
all_developers_list = all_all_devs
print(len(all_all_devs))

#@title Get scenario infomation
def running_scenarios():
  workbook = load_workbook('data/git-commits.xlsx')
  sheet = workbook['Records']
  
  date_lb = datetime.datetime.strptime("2018-07-13 00:00:00","%Y-%m-%d %H:%M:%S") # lower boundary
  date_ub = datetime.datetime.strptime("2019-03-17 23:59:59","%Y-%m-%d %H:%M:%S") # upper boundary
  print(date_lb, date_ub, "\n")

  bug_creation_dates = []
  list_of_devs = []
  kludges = 0
  fixes = 0
  for i in range(2, len(sheet['D']), 1): # column D is for commit date
    bug_creation_date = sheet['D'+str(i)].value[4:-6] # removing the day of the week and the time-zone
    date_formatted = datetime.datetime.strptime(bug_creation_date,"%b %d %H:%M:%S %Y") # formatted dates
      
    td_ub = date_ub - date_formatted
    td_lb = date_formatted - date_lb
#     print(td_ub, td_lb.days)
      
    if td_ub.days > 0: # must be under the upper bound
      if td_lb.days > 0: # must be above the lower bound
#         print(bug_creation_date, sheet['B'+str(i)].value) # B for developers names
        bug_creation_dates.append(date_formatted)
        
        # get developer email
        value = sheet['B'+str(i)].value
        value = value.split("<")
        value = value[1].split(">")
        list_of_devs.append(value[0])
        
        if sheet['H'+str(i)].value != '': # if this commit is a bug fix
#           print("Yes")
          alert_per_line = sheet['G'+str(i)].value / sheet['I'+str(i)].value # alerts per lines added
          if alert_per_line > quality_of_code:
            kludges += 1
          else:
            fixes += 1
            
          
#         else:
#           print('Noo')
          
      else: # otherwise, stop
        break
  
  list_of_devs = list(set(list_of_devs))
#   for i in list_of_devs:
#     print(i)
  print(len(bug_creation_dates), len(list_of_devs))
  print("K:", kludges, "F:", fixes)
  
  return list_of_devs
  
rn_list_of_devs = running_scenarios()

#@title Additional statistics
def getting_overall_stats():
  workbook = load_workbook('data/git-commits.xlsx')
  sheet = workbook['Records']
  
  workbook_two = load_workbook('data/bugzilla-overview.xlsx')
  sheet_two = workbook_two['Bugs Overview']
  
  total_number_of_git_commits = len(sheet['A']) -1 # deleting the heading
  total_number_of_bugs = len(sheet_two['A']) -1 # deleting the heading
  print(total_number_of_git_commits, total_number_of_bugs)
  
  total_number_of_bugs_new = 0
  kludges = 0
  fixes = 0
  for i in range(2, len(sheet['A']), 1):
    if sheet['H'+str(i)].value != '':
      total_number_of_bugs_new += 1
      
      alert_per_line = 0
      if sheet['G'+str(i)].value == 0 and sheet['I'+str(i)].value == 0:
        alert_per_line = 0
      else:
        alert_per_line = sheet['G'+str(i)].value / sheet['I'+str(i)].value # alerts per lines added
      if alert_per_line > quality_of_code:
        kludges += 1
      else:
        fixes += 1
  
  print(total_number_of_bugs_new)
  print("Percentage of commits aimed to fix a bug:", (total_number_of_bugs_new / total_number_of_git_commits))
  print("K:", kludges, "F:", fixes)
  
getting_overall_stats()

def get_the_dev_list():
  indices = []
  for dev in all_all_devs:
    if dev[0] in rn_list_of_devs:
      indices.append(rn_list_of_devs.index(dev[0]))
  return indices
    
the_dev_list = get_the_dev_list()
print(len(the_dev_list))

#@markdown ### How many time to run the simulation?
how_many_times = 1000 #@param {type:"integer"}

if how_many_times < 0:
  print("Must be bigger than 0.")
  
#@markdown ### And for how many days?
how_many_days = 247 #@param {type:"integer"}

#@title Running the same simulation many times
day_of_simulation = []
total_bugs_of_simulation = []
# print(the_dev_list)

if use_theoretical_models == True:
  print("Doesn't support theoretical models.")

elif use_float_days == True:
  # saving total stats
  total_t, total_r, total_o, total_f, total_k = 0, 0, 0, 0, 0
  
  for i in range(how_many_times):
#     print(the_dev_list)
    env = simpy.Environment()
    bcs = simpy.Resource(env, capacity=len(the_dev_list))
    bug_simulation = BugSimulation_two(env, bcs, the_dev_list.copy())
    env.run(until=how_many_days)
  
    t, r, o, f, k = bug_simulation.return_info()
    
    total_t += t
    total_r += r
    total_o += o
    total_f += f
    total_k += k
    
  print("Average number of bugs resolved: %.2f" % (total_t / how_many_times))
  print("Average number of bugs reported: %.2f" % (total_r / how_many_times))
  print("Average number of times reopened: %.2f" % (total_o / how_many_times))
  print("Average number of bugs-fixes: %.2f" % (total_f / how_many_times))
  print("Average number of bugs-kludges: %.2f" % (total_k / how_many_times))
  
else:
  print("Doesn't support discrete data.")

